---
title: "Predicting Song popularity using pseudo random sampling of spotify catalog"
author: "prj-pcbarko-schen176--mettler3-yc62-gianghl2"
output:
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: yes
    number_sections: yes
runtime: shiny
date: '`r Sys.Date()`'
---

\newpage 
\tableofcontents 
\listoffigures
\listoftables
\newpage   
    
```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
library(caret)
library(dplyr)
#library(lmtest)
#library(randomForest)
#library(gbm)
#library(glmnet)
#library(e1071)
#library(foreach)
#library(doParallel)
#library(ranger)
#library(abcrf)
#library(xgboost)
#library(kernlab)
library(tidyverse)
#library(spotifyr)
library(MASS)
library(stringi)
library(parallel)
library(data.table)
library(plyr)
library(foreach)
library(reticulate)
library(formatR)

library(kableExtra)
library(tidyverse)
library(Hmisc)
library(janitor)
library(stringr)
library(rstatix)
library(FactoMineR)
library(factoextra)
library(reshape2)
library(corrplot)
library(shiny)
library(bookdown)
```

# Abstract

Spotify is an audio streaming service used by hundreds of millions of people. The popularity of Spotify songs is described by a popularity index and each song is associated with features that quantify various sonic attributes (e.g. pitch, daceability, etc.). For this group project, we sought to generate statsitical models that can predict the popularity of a song based on these sonic attributes. We constructed a Spotify songs database using custom R and Python scripts to query the Spotify API and appended it to a similar, publicly available database. Song popularity was predicted using several models, whose performance and runtime were benchmarked and compared.

# Introduction

[Spotify](https://open.spotify.com) is a web-based audio streaming service, first launchge in 2008, with 456 million users, including 195 million subscribers across 183 markets. The popularity of each song is quantified using a numeric popularity index from 0-100. Each song is also associated with metadata including genre, artist, and several attributes that describe varius acoustic/sonic features:

- acousticness
- danceability
- durationms
- energy
- instrumentalness
- key
- liveness
- loudness
- mode
- speechiness
- tempo
- timesignature
- valence

We sought to model the song popularity index using the acoustic attributes in a large database of Spotify songs. There are several publicly available Spotify song datasets, but these are outdates. Additionally, we sough to develop a novel means of accessing song data from the Spotify API. 

Our *fist objective* was to create a new, updated dataset of Spotify songs. We attempted to accomplish this by generating random song IDs and using these to search the Spotify API. 

Our *second objective* was to model song popularity (dependent variable) from the acoustic attributes (independet variables). Others have attempted to model popularity from the acoustic attributes and genre, but most used linear models that had poor performance. For this project, we utilized alternative approaches to modeling/predicting song popualrity from acoustic attributes and compared them with respect to performance and runtime.

# Methods

Would it be better to include a general descroiption of the methods here? Maybe a discussion of how the randome song IDs were generated and used to scrape the API?

# Results and Discussion

## Sampling Techniques

### Issues generating random sample

Randomly sampling the spotify catalog is a difficult task that is beyond the timeframe and
computing power available to our cohort. Spotify randomly assigns each music track a twenty one character id. The first character is always numeric but the remaining characters are case sensitive alphanumeric characters with repetition. Thus, we have a sample space of  $(62^{20})(10)$ but the actual sample space of spotify tracks is a much smaller subset of the total sample space (approximately $1.41^{-28}$ of the sample space). Below we implemented the stringi package to generate random id's.Unfortunately, due to the time it would take to generate a random sample this way and that the spotify API limits the number of inquiries a developer can make we would not be able to generate a sample this way. 

```{r,eval=FALSE, "Random Sampling function"}
spot_id_track_check<- function(x) get_tracks(x)

spot_ids<-function(Length){
  
st_int<- stri_rand_strings(1, 1, "[0-9]")
st_char<-stri_rand_strings(as.integer(Length), 21, pattern = "[A-Za-z0-9]")
spot_id<-seq(st_int)
for (i in spot_id){
 spot_id[i]<-paste(st_int[i],st_char[i],sep="")}

xt<-do.call(c,replicate(n=Length,mclapply(spot_id,function(x){spot_id_track_check(x)},mc.cores = 48)))
return(xt)
}  
songs<-spot_ids("1000")
songs

```

We decided to use the function _search_spotify_ to return tracks from randomly generated strings.The function allows developers to search Spotify tracks by matching strings. Below are several examples of pseudo random samples using shell script, R, and Python. 

\newpage 
### Bash sampling script
```{bash, engine.opts='-l',eval=FALSE}

# Bash script to search for random ids for Spotify tracks using the web API.
# STAT 447
# Giang Le
# Usage: bash spotify_scraping.sh $CLIENT_ID $CLIENT_SECRET
# bash spotify_scraping.sh 7c83827741ee4cbc9bd117a9cf608a39 9c2a651fdabd472892b873644f774b09

#CLIENT_ID=$1
#CLIENT_SECRET=$2

#creds=$CLIENT_ID:$CLIENT_SECRET
#encoded_creds=$(echo -n $creds | base64)

access_token=$(curl -s -X "POST" -H "Authorization: Basic $encoded_creds" -d grant_type=client_credentials https://accounts.spotify.com/api/token | awk -F"\"" '{print $4}')
echo $access_token
rm result.json extracted_ids.txt
for x in {a..z}
do
curl --request GET --url "https://api.spotify.com/v1/search?q=%25"$x"%25&type=track&limit=50&market=US" --header "Authorization: Bearer $access_token" --header "Content-Type: application/json" >> result.json
done

cat result.json | grep -o "\"id\" :.*" | sed 's/"id" : "//g' | sed 's/",$//g' >> extracted_ids.txt




```

### R sampling script
```{r, eval=FALSE}
# R script to sample tracks using spotifyr package


# function ran inside mylist function to get audio features of sampled tracks
spot_id_track_check<-function(x) get_track_audio_features(x)  

mylist<-function(x){
  xL<-c()
  st_char<-stri_rand_strings(2, x, pattern = "[A-Za-z0-9]")## generate random strings to sample from spotify search
  xL<-foreach(i=st_char) %do% {  
    #search track returns tracks from spotify API
    search_spotify(
      i,
      type = "track",
      market = NULL,
      limit = 50,
      offset=0,
      include_external = NULL,
      include_meta_info = FALSE)
    
  }
  xt<-xL
  # bind 
  sp_s<-bind_rows(xt, .id = "column_label")
  x_id<-sp_s$id
  
  ## get audio track features
  sp_f<-spot_id_track_check(x_id)  
  
  ## return two dataframes,track information and audio features 
  return(list(sp_s,sp_f))
}
sp_feat<-mylist(x)

### Replicate function to get large dataset of spotify tracks
x<-replicate(n=200, mclapply(3,function(x){mylist(x)},mc.cores = 48))

# remove NA
sp_f2<-x[!(is.na(x ))]

## Beind and seperate dataframes to then reincorporate into one dataframe
sp_s1<-bind_rows(x[1:200],.id = "column_label")
sp_s2<-sp_s1[!(is.na(sp_s1$explicit )),]
sp_s2<-sp_s2[,1:30]


sp_f2<-sp_s1[(is.na(sp_s1$explicit)),]
sp_f2<-sp_f2[,31:44]

sp_all<-cbind(sp_s2,sp_f2)

sp_s2[3,]
sp_f2[3,]
SP_ALL<-sp_all

#########################################
#Check if all track id's are unique, if not second line removes duplicates
length(unique(SP_ALL$id))
sp_all_unique<-SP_ALL[!duplicated(SP_ALL[8]),]

#######################################



```


### Python sampling script
```{python, eval=FALSE}

# Python script to sample tracks

import csv
import itertools
import string

import pandas as pd
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials


class CrawlerSpotify:
    def __init__(self, client_id, client_secret):
        # authorization information
        self.id = client_id
        self.secret = client_secret
        self.spotify = spotipy.Spotify(client_credentials_manager=
                                       SpotifyClientCredentials(client_id=self.id, client_secret=self.secret))

    def save_tracks(self, output_path: str = None):
        search_words = self._generate_search_words()  # generate search words

        results = []
        for sq in search_words:  # iterate search words
            search_results = self.spotify.search(q=sq, type='track', limit=50)
            total = search_results['tracks']['total']
            print(f'===== A total of {total} results from search word: {sq} =====')

            for offset in range(0, total if total < 1000 else 1000, 50):  # iterate pages.
                # for each search word, the API returns 1000 results at most,
                # so we can get maximum 20 pages for each search word (50 tracks each page)
                # see https://developer.spotify.com/documentation/web-api/reference/#/operations/search
                print(f'results from page {int(offset / 50 + 1)}')
                tracks = self._parse_tracks(self.spotify.search(q=sq, type='track', limit=50, offset=offset))
                audio_features = self.spotify.audio_features([track['track_id'] for track in tracks])  # audio features
                result = [{**track, **self._parse_audio_features(audio_feature)} if audio_feature else track for
                          track, audio_feature in
                          zip(tracks, audio_features)]  # combine track details and audio features
                self._save_csv(result, output_path)  # save to csv
                results.append(result)

        return pd.DataFrame(results)

    @staticmethod
    def _save_csv(result, output_path: str):
        csv_header = ['track_id', 'track_name', 'track_artist', 'track_popularity', 'track_album_id',
                      'track_album_name', 'track_album_release_date', 'danceability', 'energy', 'key',
                      'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness',
                      'valence', 'tempo', 'duration_ms']

        with open(output_path, 'a', newline='', encoding='utf-8-sig') as fp:
            csv_writer = csv.DictWriter(fp, csv_header)
            if fp.tell() == 0:
                csv_writer.writeheader()
            csv_writer.writerows(result)

    @staticmethod
    def _parse_tracks(search_results):
        """
        extract useful data from search results
        """
        details = []
        for track in search_results['tracks']['items']:
            details.append({
                'track_id': track['id'],
                'track_name': track['name'],
                'track_artist': track['artists'][0]['name'],
                'track_popularity': track['popularity'],
                'track_album_id': track['album']['id'],
                'track_album_name': track['album']['name'],
                'track_album_release_date': track['album']['release_date']
            })
        return details

    @staticmethod
    def _parse_audio_features(audio_feature_results):
        """
        delete unnecessary data from audio feature results
        """
        del_key = ['type', 'uri', 'track_href', 'analysis_url', 'time_signature', 'id']
        return {key: audio_feature_results[key] for key in audio_feature_results if key not in del_key}

    @staticmethod
    def _generate_search_words():
        """
        generate search words. from letters a-z and numbers 0-9, with years 1985-2022
        """
        search_words = []
        for word, year in itertools.product(list(string.ascii_lowercase) + list(range(0, 10)), list(range(1985, 2023))):
            search_words.append(f'{word} year:{year}')
        return search_words


if __name__ == '__main__':
    my_client_id = 'Your Client ID'
    my_client_secret = 'Your Client Secret'
    my_output_path = r'result_spotify.csv'

    crawler = CrawlerSpotify(client_id=my_client_id, client_secret=my_client_secret)
    data_tracks = crawler.save_tracks(output_path=my_output_path)  # 1,368,207 tracks

    # delete duplicates
    data = pd.read_csv(my_output_path)
    data_final = data.drop_duplicates().reset_index(drop=True)  # 573,131 tracks
    data_final.to_csv(r'result_spotify_noduplicates.csv', index=False, encoding='utf-8-sig')
    # data_final['loudness'].isna().sum()  # 615 tracks can not extract audio features

```
\newpage 

## Data Preparation and Cleaning

Scraping the Spotify API resulted in the generation the three datasets of Spotify Songs. These datasets were cleaned and combined to generate a single database of songs.

```{r}

list.files()
songs <- read.delim("Combine_UniqueTracks.txt", sep = ",")

songs2 <- read.csv("data_part2.txt")

songs3 <- read.csv("data_part1.txt")

```

Synchronize column names and order:

```{r}
colnames(songs)

table(colnames(songs2) == colnames(songs3))

```

```{r}
songs <- songs[, -c(1, 18)]
songs2 <- songs2[, -c(1, 6:8)]
songs3 <- songs3[, -c(1, 6:8)]
```

```{r}
names(songs2) <- colnames(songs)

names(songs3) <- colnames(songs)

```

Combine all datasets:

```{r}
songs_combined <- rbind(songs, songs2, songs3)
```

Scan for duplicates:

```{r}
dupes <- songs_combined %>% 
  janitor::get_dupes(TrackID)

#nrow(dupes)

```

There were `r nrow(dupes)/2` duplicate track IDs in the combined dataset. These duplicated rows were removed. 

```{r}
songs_combined <- songs_combined %>%
  filter(duplicated(TrackID) == FALSE)
```

Confirm that duplicate rows have been removed: 

```{r}
songs_combined %>% janitor::get_dupes(TrackID) %>% nrow
```

The "mode" attribute codes whether the song is written in a major or minor key and is stored as a binary numeric variable (0 = minor; 1 = mojor). For interpretability, this attribute was decoded.

```{r}
songs_combined$Mode <- case_when(songs_combined$Mode == 1 ~ "Major",
                                songs_combined$Mode == 0 ~ "Minor")
```

Similarly, each song's key is stored as a numeric variable and was decoded for interpretability:

```{r}

songs_combined$Key <- case_when(
  songs_combined$Key == 0 ~ "C",
  songs_combined$Key == 1 ~ "C#/Db",
  songs_combined$Key == 2 ~ "D",
  songs_combined$Key == 3 ~ "D#/Eb",
  songs_combined$Key == 4 ~ "E",
  songs_combined$Key == 5 ~ "F",
  songs_combined$Key == 6 ~ "F#/Gb",
  songs_combined$Key == 7 ~ "G",
  songs_combined$Key == 8 ~ "G#/Ab",
  songs_combined$Key == 9 ~ "A",
  songs_combined$Key == 10 ~ "A#/Bb",
  songs_combined$Key == 11 ~ "B"
)

```

Finally, the popularity index was quantized in order to convert it from a numeric to a categorical variable. This will be useful for classification models that require categorical independent variables. 

```{r}

songs_combined <- songs_combined %>% 
  mutate(Popularity_quantized = cut(Popularity, breaks=c(0, 25, 50, 75, 100), 
                                    include.lowest = TRUE))

#table(songs_combined$Popularity_quantized)

songs_combined$Popularity_quantized <- factor(songs_combined$Popularity_quantized)

#levels(songs_combined$Popularity_quantized)

```

Finally, remove rows with NA values

```{r}
songs_combined <- songs_combined %>% drop_na()
```


## Exploratory Analysis

There were `r nrow(songs_combined)` songs from `r length(unique(songs_combined$Artist))` artists in the combined Spotify Songs dataset. 

### Song Popularity

Distribution of song popularity indices:

```{r}
hist(songs_combined$Popularity, main = "Distribution of Song Popularity Indices")
```

Here is Shiny capability to try out

```{r}
sliderInput("bins", "Number of bins:", 30, min = 1, max = 50)

renderPlot({
  x    = songs_combined$Popularity 
  bins = seq(min(x), max(x), length.out = input$bins + 1)

  # draw the histogram with the specified number of bins
  hist(x, breaks = bins, col = 'darkgray', border = 'white')
})
```

Distribution of quantized popularity indices:

```{r}
quant_pop <- data.frame(table(songs_combined$Popularity_quantized))

names(quant_pop)[1] <- "Quantized Popularity"

ggplot(quant_pop, aes(x=`Quantized Popularity`, 
                      y = Freq, 
                      fill = `Quantized Popularity`)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Freq), vjust=0) +
  theme_bw() +
  ggtitle("Quantized Popularity Frequencies")

```

```{r}
kable(quant_pop)
```


Clearly, popular songs are rare in this dataset and the majority of songs had low popularity. 

The top twenty most popular songs:


```{r}
songs_combined %>% 
  arrange(desc(Popularity)) %>% 
  dplyr::select(SongName, Artist, Popularity) %>% 
  top_n(20) %>% kable 
```


Now, the interactive, shiny version of the table - the user can select how many songs to look at.

```{r, echo = FALSE}
numericInput("rows", "How Many Songs?", 5)

renderTable({
  songs_combined %>% 
  arrange(desc(Popularity)) %>% 
  dplyr::select(SongName, Artist, Popularity) %>% 
  top_n(input$rows)
})

```

## Acoustic Attributes

Distributions of acoustic variables (raw data):

```{r}

hist.data.frame(songs_combined[, c(5:6, 8, 10:16)]) 

```


### Principal Component Analysis

```{r, include=F}

num_cols <- unlist(lapply(songs_combined, is.numeric))
                   
pca <- PCA(songs_combined[, c(5:6, 8, 10:16)], graph = F, scale.unit = T)

fviz_eig(pca)

summary(pca)

#summary(pca)

```


```{r}
#format pca data for plotting

dat_pca <- data.frame(pca$ind$coord[, 1], pca$ind$coord[, 2])

names(dat_pca) <- c("PC1", "PC2")

pca.vars <- pca$var$coord %>% data.frame

pca.vars$vars <- rownames(pca.vars)

pca.vars.m <- melt(pca.vars, id.vars = "vars")

dat_pca$Popularity_quantized <- factor(songs_combined$Popularity_quantized)

dat_pca <- dat_pca %>% na.omit()

ggplot(data = dat_pca, aes(x = PC1, y = PC2, color = Popularity_quantized)) +
  scale_color_manual(values = c("blue", "orange", "green", "purple")) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = 0, lty = 2) +
  guides(color = guide_legend(title = "Song Popularity (Quantized)")) +
  scale_shape_manual(values = c(15, 16, 16, 17, 18)) +
  geom_point(alpha = 0.5, size = 1) + 
  xlab("PC 1 (33.3%)") + 
  ylab("PC 2 (14.7%)") +
  facet_wrap(vars(Popularity_quantized)) +
  theme_minimal() +
  theme(panel.grid = element_blank(), 
        panel.border = element_rect(fill= "transparent")) +
  theme(legend.position="bottom") + 
  theme(legend.title=element_text(size=14), 
    legend.text=element_text(size=12)) +
  ggtitle("PCA of Acoustic Attributes")


```

Because there are so many datapoints, the PCA plots are faceted by the quantized popularity indices. 


Interactive Shiny PCA plot:

```{r}

fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('group', 'Popularity Index (Quantized) Group', dat_pca$Popularity_quantized)),
        column(4, selectInput('color', 'Group Color', c("blue", "orange", "green", "purple"))))


renderPlot({
  ggplot(data = dat_pca[dat_pca$Popularity_quantized == input$group, ], aes(x = PC1, y = PC2, 
                                                                            color = Popularity_quantized)) +  
  geom_point(alpha = 0.5, size = 1) + 
  scale_color_manual(values = input$color) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = 0, lty = 2) +
  scale_shape_manual(values = c(15, 16, 16, 17, 18)) +
  xlab("PC 1 (33.3%)") + 
  ylab("PC 2 (14.7%)") +
  theme_minimal() +
  theme(panel.grid = element_blank(), 
        panel.border = element_rect(fill= "transparent")) +
  theme(legend.position="bottom") + 
  theme(legend.title=element_text(size=14), 
    legend.text=element_text(size=12)) +
  ggtitle("PCA of Acoustic Attributes (Interactive)")

  
})

```


### Correlation Analysis

Here we sought to determine if sonic attributes are correlated with song popularity.

```{r}

#this takes a couple of minutes to run

songs_combined[, c(4, 5:6, 8, 10:16)] %>%
  gather(-Popularity, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = Popularity)) +
    geom_point(size = 1) +
    facet_wrap(~ var, scales = "free") +
    theme_bw()
```
Most acoustic variables do not appear to be correlated with song popularity. However a few possible relationships are apparent:

 - Dancability is positively correlated with popularity
 - Duration is negatively correlated with popularity
 - Loudness is positivelt correlated with popularity
 - Speachiness is negatively correlated with popularity
 
Pearson Correlation Coefficients:

```{r}

cor.res <- songs_combined[, c(4, 5:6, 8, 10:16)] %>% 
  cor_test(Popularity, method = "pearson")

cor.res$FDR <- qvalue::qvalue(cor.res$p, pi0 = 1)$qvalues

cor.res

#cor.res[cor.res$FDR<0.05, c(1:3, 5, 9)] %>% arrange(cor)
```



Plot correlations among all acoustic variables and popularity (Pearson)

```{r}

all_cor <- cor(songs_combined[, c(4, 5:6, 8, 10:16)], method = "pearson")

corrplot(all_cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

```

\newpage 

## Modeling Analysis

Using the whole dataset, the models take too long to run. With greater computational resources, the models can be run on the entire dataset. For the purposes of this project, the models were run on a smaller subset of rows selected randomly from the whole data. 


```{r}

songs_subset <- slice_sample(songs_combined, n=10000)
```


### Regression models

####  1) Random Forest
####  2) Gradient Boosting Machine
####  3) Ridge Regression
####  4) Lasso Regression
####  5) Support Vector Regression

\newpage 
### Classification Models

####  1) Random Forest
####  2) Gradient Boosting Machine
####  3) K-Nearest Neighbors
####  4) Linear Discriminant Analysis
####  5) Support Vector Regression

\newpage 
## Results

\newpage 
## Shiny app?

\newpage 
## Conclusion

\newpage 
## References